{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "def load_variables():\n",
    "    \"\"\"Load authentication details\"\"\"\n",
    "    env_var=load_dotenv('./variables.env')\n",
    "    auth_dict = {\n",
    "            \"subscription_id\":os.environ['SUB_ID'],\n",
    "            \"resource_group\":os.environ['RESOURCE_GROUP'],\n",
    "            \"workspace\":os.environ['WORKSPACE_NAME'],\n",
    "            }\n",
    "    return auth_dict\n",
    "\n",
    "auth_var = load_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "import-mlclient"
   },
   "outputs": [],
   "source": [
    "# Authentication package\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "ml_client"
   },
   "outputs": [],
   "source": [
    "# Handle to the workspace\n",
    "from azure.ai.ml import MLClient\n",
    "# Get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    credential=credential,\n",
    "    subscription_id=auth_var['subscription_id'],\n",
    "    resource_group_name=auth_var['resource_group'],\n",
    "    workspace_name=auth_var['workspace'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "name": "credit_data"
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import Data\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "web_path = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00350/default%20of%20credit%20card%20clients.xls\"\n",
    "\n",
    "credit_data = Data(\n",
    "    name=\"creditcard_defaults\",\n",
    "    path=web_path,\n",
    "    type=AssetTypes.URI_FILE,\n",
    "    description=\"Dataset for credit card defaults\",\n",
    "    tags={\"source_type\": \"web\", \"source\": \"UCI ML Repo\"},\n",
    "    version=\"1.0.0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "update-credit_data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with name creditcard_defaults was registered to workspace, the dataset version is 1.0.0\n"
     ]
    }
   ],
   "source": [
    "credit_data = ml_client.data.create_or_update(credit_data)\n",
    "print(f\"Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "name": "cpu_cluster"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating a new cpu compute target...\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import AmlCompute\n",
    "\n",
    "cpu_compute_target = \"cpu-cluster\"\n",
    "\n",
    "try:\n",
    "    # let's see if the compute target already exists\n",
    "    cpu_cluster = ml_client.compute.get(cpu_compute_target)\n",
    "    print(\n",
    "        f\"You already have a cluster named {cpu_compute_target}, we'll reuse it as is.\"\n",
    "    )\n",
    "\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "\n",
    "    # Let's create the Azure ML compute object with the intended parameters\n",
    "    cpu_cluster = AmlCompute(\n",
    "        # Name assigned to the compute cluster\n",
    "        name=\"cpu-cluster\",\n",
    "        # Azure ML Compute is the on-demand VM service\n",
    "        type=\"amlcompute\",\n",
    "        # VM Family\n",
    "        size=\"STANDARD_DS3_V2\",\n",
    "        # Minimum running nodes when there is no job running\n",
    "        min_instances=1,\n",
    "        # Nodes in cluster\n",
    "        max_instances=4,\n",
    "        # How many seconds will the node running after the job termination\n",
    "        idle_time_before_scale_down=180,\n",
    "        # Dedicated or LowPriority. The latter is cheaper but there is a chance of job termination\n",
    "        tier=\"Dedicated\",\n",
    "    )\n",
    "\n",
    "    # Now, we pass the object to MLClient's create_or_update method\n",
    "    cpu_cluster = ml_client.begin_create_or_update(cpu_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "name": "dependencies_dir"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dependencies_dir = \"./dependencies\"\n",
    "os.makedirs(dependencies_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "conda.yml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./dependencies/conda.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {dependencies_dir}/conda.yml\n",
    "name: model-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.8\n",
    "  - numpy=1.21.2\n",
    "  - pip=21.2.4\n",
    "  - scikit-learn=0.24.2\n",
    "  - scipy=1.7.1\n",
    "  - pandas>=1.1,<1.2\n",
    "  - pip:\n",
    "    - inference-schema[numpy-support]==1.3.0\n",
    "    - xlrd==2.0.1\n",
    "    - mlflow== 1.26.1\n",
    "    - azureml-mlflow==1.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "custom_env_name"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment with name aml-scikit-learn is registered to workspace, the environment version is 1.0\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.ml.entities import Environment\n",
    "\n",
    "custom_env_name = \"aml-scikit-learn\"\n",
    "\n",
    "pipeline_job_env = Environment(\n",
    "    name=custom_env_name,\n",
    "    description=\"Custom environment for Credit Card Defaults pipeline\",\n",
    "    tags={\"scikit-learn\": \"0.24.2\"},\n",
    "    conda_file=os.path.join(dependencies_dir, \"conda.yml\"),\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    "    version=\"1.0\",\n",
    ")\n",
    "pipeline_job_env = ml_client.environments.create_or_update(pipeline_job_env)\n",
    "\n",
    "print(\n",
    "    f\"Environment with name {pipeline_job_env.name} is registered to workspace, the environment version is {pipeline_job_env.version}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "data_prep_src_dir"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "data_prep_src_dir = \"./components/data_prep\"\n",
    "os.makedirs(data_prep_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "def-main"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/data_prep/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {data_prep_src_dir}/data_prep.py\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--data\", type=str, help=\"path to input data\")\n",
    "    parser.add_argument(\"--test_train_ratio\", type=float, required=False, default=0.25)\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Start Logging\n",
    "    mlflow.start_run()\n",
    "\n",
    "    print(\" \".join(f\"{k}={v}\" for k, v in vars(args).items()))\n",
    "\n",
    "    print(\"input data:\", args.data)\n",
    "\n",
    "    credit_df = pd.read_excel(args.data, header=1, index_col=0)\n",
    "\n",
    "    mlflow.log_metric(\"num_samples\", credit_df.shape[0])\n",
    "    mlflow.log_metric(\"num_features\", credit_df.shape[1] - 1)\n",
    "\n",
    "    credit_train_df, credit_test_df = train_test_split(\n",
    "        credit_df,\n",
    "        test_size=args.test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # output paths are mounted as folder, therefore, we are adding a filename to the path\n",
    "    credit_train_df.to_csv(os.path.join(args.train_data, \"data.csv\"), index=False)\n",
    "\n",
    "    credit_test_df.to_csv(os.path.join(args.test_data, \"data.csv\"), index=False)\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "name": "data_prep_component"
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml import command\n",
    "from azure.ai.ml import Input, Output\n",
    "\n",
    "data_prep_component = command(\n",
    "    name=\"data_prep_credit_defaults\",\n",
    "    display_name=\"Data preparation for training\",\n",
    "    description=\"reads a Excel input, split the input to train and test\",\n",
    "    inputs={\n",
    "        \"data\": Input(type=\"uri_folder\"),\n",
    "        \"test_train_ratio\": Input(type=\"number\"),\n",
    "    },\n",
    "    outputs=dict(\n",
    "        train_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "        test_data=Output(type=\"uri_folder\", mode=\"rw_mount\"),\n",
    "    ),\n",
    "    # The source folder of the component\n",
    "    code=data_prep_src_dir,\n",
    "    command=\"\"\"python data_prep.py \\\n",
    "            --data ${{inputs.data}} --test_train_ratio ${{inputs.test_train_ratio}} \\\n",
    "            --train_data ${{outputs.train_data}} --test_data ${{outputs.test_data}} \\\n",
    "            \"\"\",\n",
    "    environment=f\"{pipeline_job_env.name}:{pipeline_job_env.version}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train_src_dir"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_src_dir = \"./components/train\"\n",
    "os.makedirs(train_src_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train.py"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/train/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.py\n",
    "import argparse\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import os\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "\n",
    "\n",
    "def select_first_file(path):\n",
    "    \"\"\"Selects first file in folder, use under assumption there is only one file in folder\n",
    "    Args:\n",
    "        path (str): path to directory or file to choose\n",
    "    Returns:\n",
    "        str: full path of selected file\n",
    "    \"\"\"\n",
    "    files = os.listdir(path)\n",
    "    return os.path.join(path, files[0])\n",
    "\n",
    "\n",
    "# Start Logging\n",
    "mlflow.start_run()\n",
    "\n",
    "# enable autologging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "os.makedirs(\"./outputs\", exist_ok=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function of the script.\"\"\"\n",
    "\n",
    "    # input and output arguments\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--train_data\", type=str, help=\"path to train data\")\n",
    "    parser.add_argument(\"--test_data\", type=str, help=\"path to test data\")\n",
    "    parser.add_argument(\"--n_estimators\", required=False, default=100, type=int)\n",
    "    parser.add_argument(\"--learning_rate\", required=False, default=0.1, type=float)\n",
    "    parser.add_argument(\"--registered_model_name\", type=str, help=\"model name\")\n",
    "    parser.add_argument(\"--model\", type=str, help=\"path to model file\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    train_df = pd.read_csv(select_first_file(args.train_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_train = train_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_train = train_df.values\n",
    "\n",
    "    # paths are mounted as folder, therefore, we are selecting the file from folder\n",
    "    test_df = pd.read_csv(select_first_file(args.test_data))\n",
    "\n",
    "    # Extracting the label column\n",
    "    y_test = test_df.pop(\"default payment next month\")\n",
    "\n",
    "    # convert the dataframe values to array\n",
    "    X_test = test_df.values\n",
    "\n",
    "    print(f\"Training with data of shape {X_train.shape}\")\n",
    "\n",
    "    clf = GradientBoostingClassifier(\n",
    "        n_estimators=args.n_estimators, learning_rate=args.learning_rate\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Registering the model to the workspace\n",
    "    print(\"Registering the model via MLFlow\")\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=clf,\n",
    "        registered_model_name=args.registered_model_name,\n",
    "        artifact_path=args.registered_model_name,\n",
    "    )\n",
    "\n",
    "    # Saving the model to a file\n",
    "    mlflow.sklearn.save_model(\n",
    "        sk_model=clf,\n",
    "        path=os.path.join(args.model, \"trained_model\"),\n",
    "    )\n",
    "\n",
    "    # Stop Logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train.yml"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./components/train/train.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile {train_src_dir}/train.yml\n",
    "name: train_credit_defaults_model\n",
    "display_name: Train Credit Defaults Model\n",
    "version: 1 # Not specifying a version will automatically update the version\n",
    "type: command\n",
    "inputs:\n",
    "  train_data: \n",
    "    type: uri_folder\n",
    "  test_data: \n",
    "    type: uri_folder\n",
    "  learning_rate:\n",
    "    type: number     \n",
    "  registered_model_name:\n",
    "    type: string\n",
    "outputs:\n",
    "  model:\n",
    "    type: uri_folder\n",
    "code: .\n",
    "environment:\n",
    "  # for this step, we'll use an AzureML curate environment\n",
    "  azureml:AzureML-sklearn-0.24-ubuntu18.04-py37-cpu:21\n",
    "command: >-\n",
    "  python train.py \n",
    "  --train_data ${{inputs.train_data}} \n",
    "  --test_data ${{inputs.test_data}} \n",
    "  --learning_rate ${{inputs.learning_rate}}\n",
    "  --registered_model_name ${{inputs.registered_model_name}} \n",
    "  --model ${{outputs.model}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "train_component"
   },
   "outputs": [],
   "source": [
    "# importing the Component Package\n",
    "from azure.ai.ml import load_component\n",
    "\n",
    "# Loading the component from the yml file\n",
    "train_component = load_component(source=os.path.join(train_src_dir, \"train.yml\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "update-train_component"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading train (0.0 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3379/3379 [00:00<00:00, 42357.72it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Component train_credit_defaults_model with Version 1 is registered\n"
     ]
    }
   ],
   "source": [
    "# Now we register the component to the workspace\n",
    "train_component = ml_client.create_or_update(train_component)\n",
    "\n",
    "# Create (register) the component in your workspace\n",
    "print(\n",
    "    f\"Component {train_component.name} with Version {train_component.version} is registered\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "pipeline"
   },
   "outputs": [],
   "source": [
    "# the dsl decorator tells the sdk that we are defining an Azure ML pipeline\n",
    "from azure.ai.ml import dsl, Input, Output\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "    compute=cpu_compute_target,\n",
    "    description=\"E2E data_perp-train pipeline\",\n",
    ")\n",
    "def credit_defaults_pipeline(\n",
    "    pipeline_job_data_input,\n",
    "    pipeline_job_test_train_ratio,\n",
    "    pipeline_job_learning_rate,\n",
    "    pipeline_job_registered_model_name,\n",
    "):\n",
    "    # using data_prep_function like a python call with its own inputs\n",
    "    data_prep_job = data_prep_component(\n",
    "        data=pipeline_job_data_input,\n",
    "        test_train_ratio=pipeline_job_test_train_ratio,\n",
    "    )\n",
    "\n",
    "    # using train_func like a python call with its own inputs\n",
    "    train_job = train_component(\n",
    "        train_data=data_prep_job.outputs.train_data,  # note: using outputs from previous step\n",
    "        test_data=data_prep_job.outputs.test_data,  # note: using outputs from previous step\n",
    "        learning_rate=pipeline_job_learning_rate,  # note: using a pipeline input as parameter\n",
    "        registered_model_name=pipeline_job_registered_model_name,\n",
    "    )\n",
    "\n",
    "    # a pipeline returns a dictionary of outputs\n",
    "    # keys will code for the pipeline output identifier\n",
    "    return {\n",
    "        \"pipeline_job_train_data\": data_prep_job.outputs.train_data,\n",
    "        \"pipeline_job_test_data\": data_prep_job.outputs.test_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "registered_model_name"
   },
   "outputs": [],
   "source": [
    "registered_model_name = \"credit_defaults_model\"\n",
    "\n",
    "# Let's instantiate the pipeline with the parameters of our choice\n",
    "pipeline = credit_defaults_pipeline(\n",
    "    # pipeline_job_data_input=credit_data,\n",
    "    pipeline_job_data_input=Input(type=\"uri_file\", path=web_path),\n",
    "    pipeline_job_test_train_ratio=0.2,\n",
    "    pipeline_job_learning_rate=0.25,\n",
    "    pipeline_job_registered_model_name=registered_model_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "name": "returned_job"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading data_prep (0.0 MBs): 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1386/1386 [00:00<00:00, 30589.10it/s]\u001b[0m\n",
      "\u001b[39m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import webbrowser\n",
    "\n",
    "# submit the pipeline job\n",
    "pipeline_job = ml_client.jobs.create_or_update(\n",
    "    pipeline,\n",
    "    # Project's name\n",
    "    experiment_name=\"e2e_registered_components\",\n",
    ")\n",
    "# open the pipeline in web browser\n",
    "webbrowser.open(pipeline_job.services[\"Studio\"].endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the latest model (after the training pipeline has completed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "latest_model_version"
   },
   "outputs": [],
   "source": [
    "# Let's pick the latest version of the model\n",
    "latest_model_version = max([int(m.version) for m in ml_client.models.list(name=registered_model_name)])\n",
    "\n",
    "# Get the latest model\n",
    "model = ml_client.models.get(name=registered_model_name, version=latest_model_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create real-time endpoint and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "online_endpoint_name"
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "# Creating a unique name for the endpoint\n",
    "first_endpoint = \"basic-endpoint-\" + str(uuid.uuid4())[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "endpoint"
   },
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    ")\n",
    "\n",
    "    \n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=first_endpoint,\n",
    "    description=\"Real-time online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\n",
    "        \"training_dataset\": \"credit_defaults\",\n",
    "        \"model_type\": \"sklearn.GradientBoostingClassifier\",\n",
    "    },\n",
    ")\n",
    "\n",
    "endpoint = ml_client.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've created an endpoint, you can retrieve it as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "update-endpoint"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint \"basic-endpoint-27ba77d2\" with provisioning state \"Creating\" is retrieved\n"
     ]
    }
   ],
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=first_endpoint)\n",
    "print(f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "name": "model"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint basic-endpoint-9ce568c0 exists\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".."
     ]
    }
   ],
   "source": [
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=first_endpoint\n",
    "    model=model,\n",
    "    instance_type=\"Standard_DS3_v2\",\n",
    "    instance_count=2,\n",
    ")\n",
    "\n",
    "blue_deployment = ml_client.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create inference files for the real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove any existing json files in the test-endpoint folder\n",
    "!'./test-endpoint/remove-json.sh'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 10000, number of batches: None, remaining records: None\n",
      "Final dataframe length: 10000\n",
      "Number of records: 100000, number of batches: 3, remaining records: 10000\n",
      "Final dataframe length: 100000\n",
      "Number of records: 1000000, number of batches: 33, remaining records: 10000\n",
      "Final dataframe length: 1000000\n"
     ]
    }
   ],
   "source": [
    "%run './test-endpoint/sample_data_generator.py' --number_of_records 10000\n",
    "%run './test-endpoint/sample_data_generator.py' --number_of_records 100000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/10000_inference_data.json', '/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/100000_inference_data.json', '/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/1000000_inference_data.json']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def absoluteFilePaths(directory):\n",
    "    fp=[]\n",
    "    for dirpath,_,filenames in os.walk(directory):\n",
    "        for f in filenames:\n",
    "#             yield os.path.abspath(os.path.join(dirpath, f))\n",
    "            fp.append(os.path.abspath(os.path.join(dirpath, f)))\n",
    "    fp = [x for x in fp if '.json' in x]\n",
    "    return fp\n",
    "\n",
    "fl = absoluteFilePaths('./test-endpoint/')\n",
    "print(fl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/10000_inference_data.json', '/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/100000_inference_data.json']\n"
     ]
    }
   ],
   "source": [
    "# Filter out the largest file -> which will be used for the batch deployment\n",
    "real_time_endpoint_files = [x for x in fl if '1000000_inference_data.json' not in x]\n",
    "print(real_time_endpoint_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "name": "write-sample-request"
   },
   "outputs": [],
   "source": [
    "# %%writefile {deploy_dir}/sample-request.json\n",
    "# {\n",
    "#   \"input_data\": {\n",
    "#     \"columns\": [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22],\n",
    "#     \"index\": [0, 1],\n",
    "#     \"data\": [\n",
    "#             [20000,2,2,1,24,2,2,-1,-1,-2,-2,3913,3102,689,0,0,0,0,689,0,0,0,0],\n",
    "#             [10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 10, 9, 8]\n",
    "#         ]\n",
    "#   }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the real-time endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "attributes": {
     "classes": [
      "Python"
     ],
     "id": ""
    },
    "name": "ml_client.online_endpoints.invoke"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invoked the endpoint: basic-endpoint-9ce568c0 for file:/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/10000_inference_data.json\n",
      "Invoked the endpoint: basic-endpoint-9ce568c0 for file:/Users/userid/GithubProjects/managed-online-endpoint-testing/test-endpoint/100000_inference_data.json\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "for item in endpoint_list:\n",
    "    for file in real_time_endpoint_files:    \n",
    "        ml_client.online_endpoints.invoke(\n",
    "            endpoint_name=item,#online_endpoint_name,\n",
    "            request_file=file,#\"./deploy/inference_data.json\",\n",
    "        #     request_file = 'https://amlpipelstoragea4362a6ac.blob.core.windows.net/azureml-blobstore-1ce0bd5c-e972-4484-8bff-c01a8d5d7aaf/LocalUpload/inference_data.json',\n",
    "            deployment_name=\"blue\",\n",
    "        )\n",
    "        print(f\"Invoked the endpoint: {item} for file:{file}\")\n",
    "        time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual test\n",
    "# ml_client.online_endpoints.invoke(\n",
    "#     endpoint_name=\"basic-endpoint-5437043f\",\n",
    "#     request_file=\"./test-endpoint/10000_inference_data.json\",\n",
    "#     deployment_name=\"blue\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create batch endpoint and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.entities import BatchEndpoint\n",
    "\n",
    "bendpoint = BatchEndpoint(\n",
    "    name=\"creditops-batch\" + str(uuid.uuid4())[:8],\n",
    "    description=\"Credit operations classifier for defaults\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f77df0217f0>"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.batch_endpoints.begin_create_or_update(bendpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from azure.ai.ml.constants import BatchDeploymentOutputAction\n",
    "from azure.ai.ml.entities import BatchDeployment, BatchRetrySettings\n",
    "\n",
    "deployment = BatchDeployment(\n",
    "    name=\"classifier-random-forest\",\n",
    "#     description=\"A classifier based on XGBoost\",\n",
    "    endpoint_name=bendpoint.name,\n",
    "    model=model,\n",
    "    compute=\"cpu-cluster\",\n",
    "    instance_count=4,\n",
    "    max_concurrency_per_instance=4,\n",
    "    mini_batch_size=20,\n",
    "    output_action=BatchDeploymentOutputAction.APPEND_ROW,\n",
    "    output_file_name=\"predictions.csv\",\n",
    "    retry_settings=BatchRetrySettings(max_retries=3, timeout=300),\n",
    "    logging_level=\"info\",\n",
    ")\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f79b1c44b50>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If this references an existing cluster that is used for training, this will go through fairly quickly\n",
    "ml_client.batch_deployments.begin_create_or_update(deployment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the third file (`1000000_inference_data.csv`) which gets close to slightly <100Mb, it is advisable to load this into the blob store using AZCOPY since that will asynchronously copy that from a local storage drive. A SAS token for the default blobstore needs to be generated for the AZCOPY command to work. Then, manually or programmatically, mark it as a Data object for input into the batch deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 1000000, number of batches: 33, remaining records: 10000\n",
      "Final dataframe length: 1000000\n"
     ]
    }
   ],
   "source": [
    "# Create the file to run for batch inferences\n",
    "%run './test-endpoint/sample_data_generator.py' --number_of_records 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Scanning...\n",
      "INFO: Any empty folders will not be processed, because source and/or destination doesn't have full folder support\n",
      "\n",
      "Job e5148886-559e-a242-74d2-82966711be80 has started\n",
      "Log file is located at: /Users/userid/.azcopy/e5148886-559e-a242-74d2-82966711be80.log\n",
      "\n",
      "100.0 %, 1 Done, 0 Failed, 0 Pending, 0 Skipped, 1 Total, 2-sec Throughput (Mb/s): 1.4413\n",
      "\n",
      "\n",
      "Job e5148886-559e-a242-74d2-82966711be80 summary\n",
      "Elapsed Time (Minutes): 0.7335\n",
      "Number of File Transfers: 1\n",
      "Number of Folder Property Transfers: 0\n",
      "Total Number of Transfers: 1\n",
      "Number of Transfers Completed: 1\n",
      "Number of Transfers Failed: 0\n",
      "Number of Transfers Skipped: 0\n",
      "TotalBytesTransferred: 87937385\n",
      "Final Job Status: Completed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copy the sample file to the default blob stores\n",
    "# Get the full URL with a SAS token for the default blobstore, and include a generous list of privileges\n",
    "# Wait for this to finish\n",
    "!azcopy copy './test-endpoint/1000000_inference_data.csv' \"https://creditopstorage44a6fab42.blob.core.windows.net/azureml-blobstore-dfbc852f-833b-448a-b143-a346fea2fd58?sp=racwdl&st=2022-11-17T17:49:03Z&se=2022-11-18T01:49:03Z&spr=https&sv=2021-06-08&sr=c&sig=QM9Qcl8QYl2ga7itCzqH2EmCF7tCeWNbpgJL%2FRDUO9M%3D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # At attempt at programmatic update of the Data object, however, this did not work with SAS tokens \n",
    "# dataset_name = \"1m-inferences\"\n",
    "\n",
    "# # Give the path to the file in the default blob container\n",
    "# filename = \"1000000_inference_data.csv\"\n",
    "# data_path=\"https://creditopstorage44a6fab42.blob.core.windows.net/azureml-blobstore-dfbc852f-833b-448a-b143-a346fea2fd58/\" + filename\n",
    "\n",
    "# inf_data = Data(\n",
    "#     path=data_path,\n",
    "#     type=AssetTypes.URI_FILE,\n",
    "#     description=\"A one million inference dataset\",\n",
    "#     name=dataset_name,\n",
    "# )\n",
    "\n",
    "# ml_client.data.create_or_update(inf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MANUALLY UPDATE A DATA ASSET THROUGH THE PORTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/subscriptions/<subid>/resourceGroups/creditops1847/providers/Microsoft.MachineLearningServices/workspaces/creditops1847ws/data/khabib/versions/1\n"
     ]
    }
   ],
   "source": [
    "# After manually updating the Data object through the Portal, linking it to the source file (Data source should be 'workspaceblobstore')\n",
    "# Input the name in manually\n",
    "ds_name = 'khabib'\n",
    "inf_dataset = ml_client.data.get(name=ds_name, label=\"latest\")\n",
    "print(inf_dataset.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = Input(type=AssetTypes.URI_FILE, path=inf_dataset.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invoke the batch endpoint to setup the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch endpoint name: creditops-batchc0e61b0f\n",
      "Batch deployment name: classifier-random-forest\n",
      "Initiating the job...\n",
      "Job response: {'additional_properties': {}, 'id': '/subscriptions/<sub-id>/resourceGroups/creditops1847/providers/Microsoft.MachineLearningServices/workspaces/creditops1847ws/batchEndpoints/creditops-batchc0e61b0f/deployments/classifier-random-forest/jobs/aae7a9c8-7c17-44f6-8e20-25be4381c78a', 'name': 'aae7a9c8-7c17-44f6-8e20-25be4381c78a', 'type': 'Microsoft.MachineLearningServices/workspaces/batchEndpoints/deployments/jobs', 'properties': <azure.ai.ml._restclient.v2020_09_01_dataplanepreview.models._models_py3.BatchJob object at 0x7f79a1131310>, 'system_data': <azure.ai.ml._restclient.v2020_09_01_dataplanepreview.models._models_py3.SystemData object at 0x7f79a11318e0>}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Batch endpoint name: {bendpoint.name}\")\n",
    "print(f\"Batch deployment name: {deployment.name}\")\n",
    "print(f\"Initiating the job...\")\n",
    "job = ml_client.batch_endpoints.invoke(\n",
    "    deployment_name = deployment.name,\n",
    "    endpoint_name=bendpoint.name, \n",
    "    input=input_data)\n",
    "print(f\"Job response: {job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Name</th><th>Type</th><th>Status</th><th>Details Page</th></tr><tr><td>creditops-batchc0e61b0f</td><td>aae7a9c8-7c17-44f6-8e20-25be4381c78a</td><td>pipeline</td><td>Completed</td><td><a href=\"https://ml.azure.com/runs/aae7a9c8-7c17-44f6-8e20-25be4381c78a?wsid=/subscriptions/<sub-id>/resourcegroups/creditops1847/workspaces/creditops1847ws&amp;tid=72f988bf-86f1-41af-91ab-2d7cd011db47\" target=\"_blank\" rel=\"noopener\">Link to Azure Machine Learning studio</a></td></tr></table>"
      ],
      "text/plain": [
       "PipelineJob({'inputs': {}, 'outputs': {}, 'jobs': {}, 'component': PipelineComponent({'auto_increment_version': False, 'source': 'REMOTE.WORKSPACE.JOB', 'is_anonymous': True, 'name': 'azureml_anonymous', 'description': 'Pipeline submission for endpoint: creditops-batchc0e61b0f, deployment: classifier-random-forest.', 'tags': {}, 'properties': {}, 'id': None, 'Resource__source_path': None, 'base_path': None, 'creation_context': None, 'serialize': <msrest.serialization.Serializer object at 0x7f77df021eb0>, 'version': '1', 'latest_version': None, 'schema': None, 'type': 'pipeline', 'display_name': 'zen_feijoa_q2wfcyn2', 'is_deterministic': None, 'inputs': {}, 'outputs': {}, 'yaml_str': None, 'other_parameter': {}, 'jobs': {}, 'job_types': {}, 'job_sources': {}, 'source_job_id': None}), 'type': 'pipeline', 'status': 'Completed', 'log_files': None, 'name': 'aae7a9c8-7c17-44f6-8e20-25be4381c78a', 'description': 'Pipeline submission for endpoint: creditops-batchc0e61b0f, deployment: classifier-random-forest.', 'tags': {'azureml.batchrun': 'true', 'azureml.deploymentname': 'classifier-random-forest'}, 'properties': {'azureml.runsource': 'azureml.PipelineRun', 'runSource': 'Unavailable', 'runType': 'HTTP', 'azureml.parameters': '{\"run_max_try\":\"3\",\"run_invocation_timeout\":\"300\",\"mini_batch_size\":\"20\",\"logging_level\":\"INFO\",\"NodeCount\":\"4\",\"append_row_file_name\":\"predictions.csv\"}', 'azureml.continue_on_step_failure': 'False', 'azureml.continue_on_failed_optional_input': 'False', 'azureml.pipelineid': '9ed9d3e4-7c51-4da6-be83-7d5d19bfc583', 'azureml.pipelineComponent': 'pipelinerun', 'stages': '{\"Initialization\":null,\"Execution\":{\"StartTime\":\"2022-11-17T17:51:48.6274629+00:00\",\"EndTime\":\"2022-11-17T17:57:57.9023117+00:00\",\"Status\":\"Finished\"}}'}, 'id': '/subscriptions/<sub-id>/resourceGroups/creditops1847/providers/Microsoft.MachineLearningServices/workspaces/creditops1847ws/jobs/aae7a9c8-7c17-44f6-8e20-25be4381c78a', 'Resource__source_path': None, 'base_path': '/Users/userid/GithubProjects/managed-online-endpoint-testing', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f79b1c37fd0>, 'serialize': <msrest.serialization.Serializer object at 0x7f780227efd0>, 'display_name': 'zen_feijoa_q2wfcyn2', 'experiment_name': 'creditops-batchc0e61b0f', 'compute': None, 'services': {'Tracking': <azure.ai.ml._restclient.v2022_10_01_preview.models._models_py3.JobService object at 0x7f79b1c446d0>, 'Studio': <azure.ai.ml._restclient.v2022_10_01_preview.models._models_py3.JobService object at 0x7f79b1c44880>}, 'settings': {}, 'identity': None, 'default_code': None, 'default_environment': None})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.jobs.get(job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifact azureml://datastores/workspaceblobstore/paths/azureml/2508e05f-fb88-4de2-9eb6-5173395f73f8/score/ to .\n"
     ]
    }
   ],
   "source": [
    "ml_client.jobs.download(name=job.name, download_path=\".\", output_name=\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./predictions.csv\", \"r\") as f:\n",
    "    data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999995</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999996</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999997</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999998</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999999</th>\n",
       "      <td>1000000_inference_data.csv</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              file prediction\n",
       "0       1000000_inference_data.csv          1\n",
       "1       1000000_inference_data.csv          0\n",
       "2       1000000_inference_data.csv          0\n",
       "3       1000000_inference_data.csv          0\n",
       "4       1000000_inference_data.csv          0\n",
       "...                            ...        ...\n",
       "999995  1000000_inference_data.csv          0\n",
       "999996  1000000_inference_data.csv          0\n",
       "999997  1000000_inference_data.csv          0\n",
       "999998  1000000_inference_data.csv          0\n",
       "999999  1000000_inference_data.csv          0\n",
       "\n",
       "[1000000 rows x 2 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "import pandas as pd\n",
    "\n",
    "score = pd.DataFrame(\n",
    "    literal_eval(data.replace(\"\\n\", \",\")), columns=[\"file\", \"prediction\"]\n",
    ")\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    876033\n",
       "1    123967\n",
       "Name: prediction, dtype: int64"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score['prediction'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
